\documentclass{beamer}

\usepackage{amsmath, amssymb}
\usepackage{outlines} % For nicer nested lists




%Information to be included in the title page:
\title{Introduction to Bayesian Statistics}
\author{William Ruth}
\date{June 2022}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{What is a Parameter?}\pause

\begin{outline}
\1<+-> Target of inference  \\~\ 

\1<+-> To a Frequentist:
    \2<+-> A fixed, unknown number that exists in the world
    \2<+-> Study by repeated sampling
        \3<+-> Usually hypothetical repeated sampling\\~\ 

\1<+-> To a Bayesian:

    \2<+-> An unknown number
    \2<+-> Quantify my beliefs about it
    \2<+-> Systematically update my beliefs using data
\end{outline}
\end{frame}

\begin{frame}
\frametitle{Quantifying and Updating Beliefs}\pause

\begin{outline}
\1<+-> Represent beliefs with probability distributions
    \2<+-> E.g. I think it's twice as likely to rain tomorrow than not
    \2<+-> E.g. I think that average rainfall in June in Vancouver is around 50mm and differing by more than 20mm is unlikely
    \3<+-> Normal with mean of 50 and SD of 10 \\~\

\1<+-> Much harder to do as a Frequentist
    \2<+-> Need infinitely many days exactly like tomorrow
    \2<+-> Or infinitely many Junes \\~\

\1<+-> Update probability distribution using data and Bayes Theorem
\end{outline}

\end{frame}

\begin{frame}
    \frametitle{Conditional Probability}\pause

    \begin{outline}
        \1<+-> Consider rolling a dice
        \1<+-> What is probability of rolling a 2 given that you know the roll is even?
            \2<+-> Easy, $1/3$ \\~\

        \1<+-> Can we say something systematic here?
    \end{outline}

\end{frame}

\begin{frame}
    \frametitle{Conditional Probability}

    \begin{outline}
        \1<+-> Consider two possible outcomes, $A$ and $B$
            \2<+-> E.g. $A$ is ``roll a 2'' and $B$ is ``roll even''
        \1<+-> Want probability of $A$ given that we know $B$ occurred
            \2<+-> Write $P(A | B)$\\~\
        
        \1<+-> Define 
        \begin{align*}
            P(A | B) = \frac{P(\text{Both } A \text{ and } B)}{P(B)}
        \end{align*}
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Conditional Probability}

    \begin{outline}
        \1<+-> Back to our example
        \begin{align*}
            \action<+->{P(2 | \text{even}) &= \frac{P(2 \text{ and even})}{P(\text{even})}\\}
            \action<+->{&= \frac{P(2)}{P(\text{even})} \\}
            \action<+->{&= \frac{1/6}{3/6}\\}
            \action<+->{&= \frac{1}{3}}
        \end{align*}
    \end{outline}

\end{frame}

% \begin{frame}
%     \frametitle{Conditional Probability}

%     \begin{outline}
%         \1<+-> An important consequence:
%         \begin{align*}
%             \action<+->{P(A \text{ and } B) &= P(A | B) \cdot P(B)}
%         \end{align*}

%         \1<+-> This is especially useful with the law of total probability
%         \begin{align*}
%             \action<+->{P(A) &= P(A \text{ and } B) + P(A \text{ and not } B)}\\~\
%         \end{align*}

%         \1<+-> Let's look at a less trivial example
%     \end{outline}
% \end{frame}

% \begin{frame}
%     \frametitle{Errors in Medical Testing}

%     \begin{outline}
%         \1<+-> Last week, my wife did a screening test for whether the baby she's pregnant with will have Down syndrome
%         \1<+-> The test was positive
%         \1<+-> What is the probability that our kid will have Down syndrome?\\~\

%         \1<+-> Obviously, this depends on features of the test
%         \1<+-> Less obviously, it also depends on the proportion of babies born with Down syndrome
%     \end{outline}
% \end{frame}

% \begin{frame}
%     \frametitle{Errors in Medical Testing}

%     \begin{outline}
%         \1<+-> The test detects $88\%$ of cases
%             \2<+-> I.e.\ False negative probability is $0.12$
%         \1<+-> The test's false positive probability is $0.09$\\~\

%         \1<+-> For a woman of her age, the probability of a baby having Down syndrome is $1/637 \approx 0.0016$
%     \end{outline}
% \end{frame}


\begin{frame}
    \frametitle{Bayesian Terminology}\pause

    \begin{outline}
        \1<+-> We start with a distribution for the unknown parameter
            \2<+-> Called the \textbf{prior distribution}
            \2<+-> Denoted by $\pi(\theta)$\\~\

        \1<+-> We have some data, $X$
        \1<+-> Its distribution depends on the parameter, $\theta$
            \2<+-> Distribution of the data, given the unknown parameter, is called the \textbf{likelihood}
            \2<+-> Denoted by $L(X | \theta)$
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Terminology}

    \begin{outline}
        \1<+-> We would like to update the distribution of $\theta$ using observed data
            \2<+-> I.e.\ Get the distribution of $\theta$ given $X$\\~\
        
        \1<+-> This is called the \textbf{posterior distribution}
            \2<+-> Denoted by $\pi(\theta | X)$
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Terminology}

    \begin{outline}
        \1<+-> It can be shown that the posterior is proportional to the likelihood times the prior
            \2<+-> I.e.\ $\pi(\theta | X) \propto L(X | \theta) \cdot \pi(\theta)$\\~\

        \1<+-> The proportionality constant depends on $X$ but not on $\theta$
        \1<+-> In principle, we can get the proportionality constant by integrating the posterior over the range of $\theta$
            \2<+-> Total probability must equal 1
        \1<+-> Usually, we can just ignore the constant
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Inference}\pause

    \begin{outline}
        \1<+-> In a sense, we're done
        \1<+-> Anything you want to say about $\theta$ can be described in terms of the posterior\\~\

        \1<+-> Let's illustrate with an example
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Example: Coin Tossing}\pause

    \begin{outline}
        \1<+-> Consider an experiment done at Berkley in 2009 in which a coin was tossed 40,000 times
            \2<+-> So that we can actually do the calculations, we will just look at the first 100 flips
            \2<+-> Of the first 100 flips, 41 came up heads
        \1<+-> Our parameter, $\theta$, is the probability of getting heads
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Example: Coin Tossing}

    \begin{outline}
        \1<+-> For the sake of illustration, let's use a uniform prior
            \2<+-> $\pi(\theta) = 1$ for $\theta \in [0, 1]$\\~\

        \1<+-> Given $\theta$, the number of heads follows a binomial distribution
            \2<+-> $L(X | \theta) = \binom{100}{X} \cdot \theta^X \cdot (1 - \theta)^{100 - X}$
            \vspace{1 mm}
            \2<+-> $L(X | \theta) \propto \theta^X \cdot (1 - \theta)^{100 - X}$\\~\

        \1<+-> Posterior is proportional to likelihood times prior
            \2<+-> $\pi(\theta | X) \propto \theta^X \cdot (1 - \theta)^{100 - X} \cdot 1$
            \2<+-> $\pi(\theta | X) = \theta^X \cdot (1 - \theta)^{100 - X}$
    \end{outline}
\end{frame}


\begin{frame}
    \frametitle{Example: Coin Tossing}

    \begin{outline}
        \1<+-> Up to proportionality constants, this posterior matches a beta distribution with parameters $X$ and $100 - X$
        \1<+-> Given data, $\theta$ follows a beta distribution with these parameter values
            \2<+-> We write $\theta|X \sim $ Beta$(X, 100-X)$\\~\

        \1<+-> Let's plug in our numbers
        \1<+-> $X$ is 41, so $\theta | X \sim$ Beta$(41, 59)$
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Example: Coin Tossing}

    \begin{outline}
        \1<+-> The beta distribution is very well studied
        \1<+-> For a Beta$(\alpha, \beta)$ distribution,
            \2<+-> The mean is $\frac{\alpha}{\beta + \alpha}$
            \2<+-> The most likely value (mode) is $\frac{\alpha - 1}{\alpha + \beta - 2}$\\~\
    
        \1<+-> For our problem, mean is $0.41$ and mode is $0.408$\\~\

        \1<+-> Unfortunately, we don't always get nice posteriors
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Computation}\pause

    \begin{outline}
        \1<+-> What if we started with a less trivial distribution for $\theta$?
        \1<+-> Real world priors and likelihoods can get very complicated\\~\

        \1<+-> The mean and mode on the previous slide are obtained analytically
        \1<+-> In general, we can't do the necessary integration or optimization
        \1<+-> Instead, the posterior mean and posterior mode must be obtained numerically
            \2<+-> Let's focus on the mean
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Computation}

    \begin{outline}
        \1<+-> In general, we can approximate the mean of a distribution by averaging
            \2<+-> Given a sample, the average is a good approximation to the mean of the underlying distribution

        \1<+-> If we can generate a sample from the posterior, we can estimate the posterior mean
        \1<+-> Bayesian computation is about efficiently generating a sample from the posterior distribution 
            \2<+-> Often very computationally intensive
            \2<+-> Many tricks to improve performance
            \2<+-> Often depends on the structure of the problem
    \end{outline}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Computation}

    \begin{outline}
        \1<+-> Algorithms include:
            \2<+-> Gibbs sampling
            \2<+-> Metropolis-Hastings
            \2<+-> Approximate Bayesian Computation (ABC)
        
        \1<+-> There are also some analytic tools:
            \2<+-> Laplace approximation
            \2<+-> Variational Bayes\\~\

        \1<+-> Many, many more of both

    \end{outline}
\end{frame}


\end{document}